import pytz
from datetime import datetime
import uuid
import json
import requests
from io import BytesIO
import numpy as np
from fastapi import HTTPException
from web3 import Web3
from sqlalchemy.orm import Session
import models, schemas
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_postgres.vectorstores import PGVector
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate
from langchain_core.output_parsers.string import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
import unicodedata

#####################################################################################
# Langchain setup: VectorStore, TextSplitter, Embeddings, LLM, ChatPrompt
#####################################################################################
connection = "postgresql+psycopg://postgres:4321@postgres_addr:5432/sillok-test"
collection_name = "sillok-test"
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large") # dim: 3072 for text-embedding-3-large
vector_store = PGVector(
    embeddings=embeddings,
    collection_name=collection_name,
    connection=connection,
    use_jsonb=True,
)
retriever = vector_store.as_retriever()
llm = ChatOpenAI(model="gpt-4.1")

# PromptTemplate ì •ì˜
prompt_template = PromptTemplate(
    input_variables=['context', 'question'],
    template=(
        "ë‹¹ì‹ ì€ ì§ˆì˜ì‘ë‹µì„ ë•ëŠ” ì¡°ìˆ˜ìž…ë‹ˆë‹¤."
        "ì•„ëž˜ì˜ ê²€ìƒ‰ëœ ë§¥ë½ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”."
        "ì •ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì•¼ í•©ë‹ˆë‹¤."
        "ë‹µë³€ì€ ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."
        "ë‹µë³€ì„ ì œê³µí•˜ê¸° ì´ì „ì— ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë…¼ë¦¬ì  ì ˆì°¨ë¥¼ êµ¬ì„±í•˜ê³  ê·¸ì— ë”°ë¼ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "ì§ˆë¬¸: {question} \në§¥ë½: {context} \nAnswer:"
    )
)

# HumanMessagePromptTemplate ì •ì˜
human_message_prompt_template = HumanMessagePromptTemplate(
    prompt=prompt_template
)

# ChatPromptTemplate ì •ì˜
custom_chat_template = ChatPromptTemplate(
    input_variables=['context', 'question'],
    messages=[human_message_prompt_template]
)
# ë§¥ë½ í¬ë§·íŒ…ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
# ìµœì¢… RAG chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_chat_template
    | llm
    | StrOutputParser()
)
#####################################################################################
# IPFS setup
#####################################################################################
IPFS_HOST = "ipfs" # Check `docker-compose.yml`
IPFS_PORT = 5001

#### IPFS + requests í…ŒìŠ¤íŠ¸ ì½”ë“œ
## ì•„ëž˜ íŒŒì¼ 1íšŒ ìž‘ì„±í•´ì„œ ì˜¬ë ¤ë‘¬ë´„.
# with open('example.md', 'rb') as file:
#     response = requests.post(
#         f'http://{IPFS_HOST}:{IPFS_PORT}/api/v0/add',
#         files={'file': file}
#     )
# print(response.json())
## ì¶œë ¥
# cid = "QmV1VCeLMQxgtEo3234KtVukK3maL79syJ9pmXMqXgvbLN"
# response = requests.post(f"http://{IPFS_HOST}:{IPFS_PORT}/api/v0/cat?arg={cid}")
# print(response.content.decode('utf-8'))
####

#####################################################################################
# Blockchain setup
#####################################################################################
ganache_address = "http://ganache_url:8545"
web3 = Web3(Web3.HTTPProvider(ganache_address))

## ë¹Œë“œ ì•„í‹°íŒ©íŠ¸ íŒŒì¼ ê²½ë¡œ
build_artifact_path = "/app/ganache_build/contracts/MetaDataStoreContract.json"

## ABI ë° ì»¨íŠ¸ëž™íŠ¸ ì£¼ì†Œ ë¡œë“œ
with open(build_artifact_path) as f:
    contract_json = json.load(f)
    contract_abi = contract_json["abi"]
    network_id = '1337'  ## Ganacheì˜ ë„¤íŠ¸ì›Œí¬ ID

    ## ì»¨íŠ¸ëž™íŠ¸ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
    if network_id in contract_json['networks']:
        contract_address = contract_json['networks'][network_id]['address']
    else:
        raise Exception(f"Contract not deployed on network {network_id}")

## Smart contract instance
contract = web3.eth.contract(address=contract_address, abi=contract_abi)

#####################################################################################
# ì•„ëž˜ëŠ” FastAPIì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ë“¤ì„ ì •ì˜í•œ ë¶€ë¶„ìž…ë‹ˆë‹¤.
#####################################################################################

def create_user(user: schemas.UserCreate, db: Session):
    # Check the user is already in the Database.
    if (db_user := db.query(models.User).filter(models.User.account == user.account).first()):
        return db_user 
    # Else create the user.
    db_user = models.User(name=user.name, account=user.account, password=user.password, email=user.email, birthday=user.birthday, gender=user.gender)
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user

# TODO: Asyncë¡œ ë³€ê²½ í•„ìš”
def create_file(request_data: schemas.FileCreate, db: Session):
    # file: user_id, fname, type, content
    global retriever # globalë¡œ ì¼ë‹¨ êµ¬í˜„í•˜ê³  ì¶”í›„ ì•ˆì •í™”ë¥¼ ìœ„í•œ ìž‘ì—…ì„ ì§„í–‰í•  ì˜ˆì •.
    
    if request_data.type != "markdown":
        # ì—¬ê¸°ì„œ ë¦¬í„´í•˜ëŠ” ìœ í˜•ì€ ì¢€ ë” ìƒê°í•´ì•¼í•¨.. ì§€ê¸ˆì€ ì¼ë‹¨ raiseë¡œ ì²˜ë¦¬.
        raise HTTPException(status_code=400, detail="Only markdown files are supported.")
    
    # # Check the file is already in the Database.
    # if (db_file := db.query(models.File).filter(models.File.CID == file.CID).first()):
    #     return db_file
    
    # 1. file_id ì¤€ë¹„
    file_id = uuid.uuid4().hex
    # 2. TextSplitterë¡œ ë¶„ë¦¬
    print("ìž…ë ¥ ë¬¸ì„œë¥¼ TextSplitterë¡œ ë¶„ë¦¬í•˜ëŠ” ì¤‘ìž…ë‹ˆë‹¤...")
    document = Document(page_content=request_data.content, metadata={"id": file_id})
    splits = text_splitter.split_documents([document])
    # 3. Embedding ë° VectorStoreì— ì €ìž¥
    print("ë¶„ë¦¬ëœ ì²­í¬ë¥¼ ìž„ë² ë”©í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ì— ì €ìž¥í•˜ëŠ” ì¤‘ìž…ë‹ˆë‹¤...")
    splits_ids = [file_id+"-"+str(i) for i in range(len(splits))]
    vector_store.add_documents(splits, ids=splits_ids)
    retriever = vector_store.as_retriever() # retriever ì—…ë°ì´íŠ¸
    # 4. Embeddingëœ ë²¡í„°ë“¤ì„ ê°€ì ¸ì˜¤ê¸°
    embedding_records = (
        db.query(models.LangchainPGEmbedding)
        .filter(models.LangchainPGEmbedding.id.in_(splits_ids))
        .all()
    )
    vectors = [list(record.embedding) for record in embedding_records]
    
    # 5. IPFSì— ì €ìž¥ (with metadata)
    print("ë°ì´í„°ë¥¼ IPFSì— ì €ìž¥í•˜ëŠ” ì¤‘ìž…ë‹ˆë‹¤...")
    metadata = {
        "fname": request_data.fname,
        "user_id": request_data.user_id,
        "last_update": datetime.now(pytz.timezone("Asia/Seoul")).strftime("%Y-%m-%d_%H:%M:%S"),
        "is_deleted": False,
        "raw_content_type": "markdown",
        "embeddings": "text-embedding-3-large",
        "text_splitter": "RecursiveCharacterTextSplitter(Langchain)",
    }
    data_for_IPFS = {
        "metadata": metadata,
        "raw_content": request_data.content,
        "splits": [{"page_content": split.page_content, "metadata": split.metadata} for split in splits],
        "split_ids": splits_ids,
        "vectors": [
            [float(v) if isinstance(v, np.float32) else v for v in vector]
            for vector in vectors
        ],
    }
    json_content = json.dumps(data_for_IPFS)
    file_like_object = BytesIO(json_content.encode('utf-8'))
    file_like_object.name = request_data.fname.split('.')[0] + ".json"
    files = {'file': (file_like_object.name, file_like_object)}
    response = requests.post(f'http://{IPFS_HOST}:{IPFS_PORT}/api/v0/add', files=files)
    result = response.json()
    # print(result) #{'Name': 'example.json', 'Hash': 'QmcjjzgPnUcSkESKCQcfHTcP1C5YBXQUPL462cTkdv4t8A', 'Size': '69284'}
    # 6. ë¸”ë¡ì²´ì¸ì— ì €ìž¥
    print("CIDì™€ ë©”íƒ€ë°ì´í„°ë“¤ì„ ë¸”ë¡ì²´ì¸ì— ì €ìž¥í•©ë‹ˆë‹¤.")
    cid = result["Hash"]
    tx_hash = contract.functions.storeFileMetadata(
        cid, # CID
        "json", # fileType
        int(datetime.now().timestamp()), # lastUpdate
        False, # isDeleted
        request_data.user_id # userId
    ).transact({
        'from': web3.eth.accounts[0],
        'gas': 2000000
    })
    print(f"ë°ì´í„°ê°€ TxHash={tx_hash.hex()} ì— ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì›ë³¸ìž„ì„ ë³´ìž¥í•©ë‹ˆë‹¤.")
    # ì„±ê³µ!
    # fastapi_app-1  | Data stored with transaction hash: 240080e13d28d6fe20e306bad3a2e9371e17b03f6e367e08665dfe21724b82b8
    # fastapi_app-1  | File created successfully.
    
    # (ì•„ëž˜) ì¶”í›„ ê²€ì¦ìš© ì½”ë“œ
    # receipt = web3.eth.wait_for_transaction_receipt(tx_hash)
    # print(f"Data stored with transaction hash: {receipt.transactionHash.hex()}")
    
    # 7. DBì— ì €ìž¥
    db_file = models.File(
        CID=cid,
        fname=request_data.fname, 
        type=request_data.type,
        last_update=metadata["last_update"],
        is_deleted=metadata["is_deleted"],
        user_id=request_data.user_id,
        TXHash=tx_hash,
        content=request_data.content # ë¹ ë¥¸ ì‚¬ìš©ì„ ìœ„í•´ CIDì™€ ë³‘í–‰
    )
    db.add(db_file)
    db.commit()
    db.refresh(db_file)
    print("ì„±ê³µì ìœ¼ë¡œ ì €ìž¥í–ˆìŠµë‹ˆë‹¤!ðŸŽ‰")
    return db_file

def get_files_by_user_id(user_id:str, db: Session):
    records = db.query(models.File).filter(models.File.user_id.in_([user_id])).all()
    results = []
    for record in records:
        if not record.is_deleted:
            results.append(record.dict())
    return results

def get_txhash_and_cid_by_user_id(user_id:str, db: Session):
    records = db.query(models.File).filter(models.File.user_id.in_([user_id])).all()
    results = []
    for record in records:
        if not record.is_deleted:
            results.append({"CID":record.CID, "fname":record.fname, "TXHash":record.TXHash})
    return results

def delete_file_by_cid(cid:str, db: Session):
    global retriever
    db_file = db.query(models.File).filter(models.File.CID == cid).first()
    if db_file is None:
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    response = requests.post(f"http://{IPFS_HOST}:{IPFS_PORT}/api/v0/cat?arg={cid}")
    content = response.json()
    splits_ids = content["split_ids"]
    vector_store.delete(ids=splits_ids)
    retriever = vector_store.as_retriever() # retriever ì—…ë°ì´íŠ¸
    
    fname = db_file.fname
    db_file.is_deleted = True
    db.commit()
    db.refresh(db_file)
    return fname

def update_file(request_data: schemas.FileUpdate, db: Session):
    file_create_data = request_data.dict(exclude={"CID"})
    delete_file_by_cid(request_data.CID, db)
    return create_file(schemas.FileCreate(**file_create_data), db)

def check_fname_exists(fname:str, db:Session):
    records = db.query(models.File).filter(models.File.fname.in_([fname])).all()
    if len(records) > 0:
        return True
    return False

def check_decodable(text:str):
    enc_text = text.encode("utf-8")
    if unicodedata.normalize('NFC', text) == unicodedata.normalize('NFC', enc_text.decode("utf-8")):
        return True
    return False

def restore_user_files(user_id: str, db: Session):
    """Restore user's files from IPFS into the vector store."""
    global retriever
    records = (
        db.query(models.File)
        .filter(models.File.user_id == user_id, models.File.is_deleted == False)
        .all()
    )

    for record in records:
        response = requests.post(
            f"http://{IPFS_HOST}:{IPFS_PORT}/api/v0/cat?arg={record.CID}"
        )
        content = response.json()
        splits = [
            Document(page_content=s["page_content"], metadata=s["metadata"])
            for s in content.get("splits", [])
        ]
        split_ids = content.get("split_ids", [])
        vectors = content.get("vectors", [])
        if vectors:
            vector_store.add_embeddings(
                texts=[doc.page_content for doc in splits],
                embeddings=vectors,
                metadatas=[doc.metadata for doc in splits],
                ids=split_ids,
            )
        else:
            vector_store.add_documents(splits, ids=split_ids)

    retriever = vector_store.as_retriever()
